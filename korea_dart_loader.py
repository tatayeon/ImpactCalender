import requests, zipfile, io, xml.etree.ElementTree as ET, os, datetime
from bs4 import BeautifulSoup
from crawler import crawl_naver_view_titles
from rag_index import create_faiss_index_from_docs
from rag_search import rag_query_from_docs
from openai import OpenAI
import streamlit as st

clova_client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY", ""),
    base_url=os.environ.get("OPENAI_BASE_URL", "")
)

DART_API_KEY = st.secrets.get("DART_API_KEY", "")

# ================================
# âœ… 1. document.xml API í˜¸ì¶œ
# ================================
def fetch_disclosure_xml(rcept_no: str) -> str:
    """
    âœ… DART document.xml API â†’ ê³µì‹œ XML ì›ë¬¸ ë°˜í™˜
    """
    url = "https://opendart.fss.or.kr/api/document.xml"
    params = {
        "crtfc_key": DART_API_KEY,
        "rcept_no": rcept_no
    }
    try:
        res = requests.get(url, params=params, timeout=15)
        if res.status_code != 200:
            print(f"âš ï¸ ê³µì‹œ XML ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
            return ""
        return res.text
    except Exception as e:
        print(f"âš ï¸ ê³µì‹œ XML ìš”ì²­ ì˜¤ë¥˜: {e}")
        return ""

def fetch_disclosure_with_tables(rcept_no: str) -> str:
    """
    âœ… XML ê¸°ë°˜ ê³µì‹œ ë³¸ë¬¸ + í‘œ ë°ì´í„° ì¶”ì¶œ
    """
    xml_content = fetch_disclosure_xml(rcept_no)
    if not xml_content:
        return ""

    soup = BeautifulSoup(xml_content, "lxml")

    # ë³¸ë¬¸ í…ìŠ¤íŠ¸
    paragraphs = [
        tag.get_text(strip=True)
        for tag in soup.find_all(["p", "div", "span", "tt"])
        if tag.get_text(strip=True)
    ]

    # í‘œ ë°ì´í„° ì¶”ì¶œ
    table_texts = []
    for table in soup.find_all("table"):
        rows = []
        for tr in table.find_all("tr"):
            cols = [td.get_text(strip=True) for td in tr.find_all(["td", "th"])]
            if any(cols):
                rows.append("\t".join(cols))
        if rows:
            table_texts.append("\n".join(rows))

    merged_tables = "\n\n".join(table_texts)

    return f"""
    ### ë³¸ë¬¸ ###
    {'\n'.join(paragraphs[:200])}  

    ### í‘œ ë°ì´í„° ###
    {merged_tables}
    """

# ================================
# âœ… 2. ê¸´ í…ìŠ¤íŠ¸ chunk ìš”ì•½
# ================================
def chunk_text(text: str, max_chars=3500):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ max_chars ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    """
    chunks = []
    while len(text) > max_chars:
        split_idx = text[:max_chars].rfind(".")  # ë¬¸ìž¥ ë‹¨ìœ„ë¡œ ìžë¥´ê¸°
        if split_idx == -1:
            split_idx = max_chars
        chunks.append(text[:split_idx])
        text = text[split_idx:]
    if text:
        chunks.append(text)
    return chunks

def summarize_chunk_with_clova(chunk: str) -> str:
   
    prompt = f"""
    ì•„ëž˜ ê³µì‹œ ì¼ë¶€ë¥¼ **í•µì‹¬ ìš”ì•½**ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
    - í‘œ ì•ˆ ìˆ«ìžëŠ” ìœ ì§€
    - ë³´ê³ ì„œ ìž¬ë¬´ ê´€ë ¨ í•µì‹¬ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ë©´ì„œ ë¹„êµ ê°•ì¡°

    ë‚´ìš©:
    {chunk}
    """
    try:
        res = clova_client.chat.completions.create(
            model="HCX-005",
            messages=[{"role": "user", "content": prompt}]
        )
        return res.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ OpenAI chunk ìš”ì•½ ì‹¤íŒ¨: {e}")
        return chunk[:1000]  # ì‹¤íŒ¨í•˜ë©´ ì¼ë¶€ë§Œ ë°˜í™˜

def safe_summarize_large_text(full_text: str) -> str:
    """
    âœ… ë„ˆë¬´ ê¸´ ê³µì‹œëŠ” chunk ìš”ì•½ í›„ ìµœì¢… ì••ì¶•
    """
    if len(full_text) < 4000:
        return full_text  # ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©

    chunks = chunk_text(full_text, max_chars=3500)
    partial_summaries = []
    for i, chunk in enumerate(chunks, 1):
        print(f"ðŸ“ Chunk {i}/{len(chunks)} ìš”ì•½ ì¤‘...")
        summary = summarize_chunk_with_clova(chunk)
        partial_summaries.append(summary)

    # âœ… partial ìš”ì•½ì„ ë‹¤ì‹œ ì••ì¶•
    final_prompt = f"""
    ì•„ëž˜ ì—¬ëŸ¬ ê°œì˜ ìš”ì•½ì„ **í•œ ë¬¸ì„œë¡œ í†µí•© ìš”ì•½**í•´ì¤˜.
    - ìž¬ë¬´ ìˆ«ìžëŠ” ìœ ì§€ ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ ë³€í™”ìœ¨ ê°•ì¡°
    - ì´ˆë³´ íˆ¬ìžìžê°€ ì´í•´í•  ìˆ˜ ìžˆëŠ” 3~4ì¤„ ìš”ì•½
    - í•µì‹¬ ìš”ì•½ë§Œ ë‚¨ê¸°ê³  ë¶ˆí•„ìš”í•œ ë¬¸ìž¥ì€ ì œê±°

    {'\n\n'.join(partial_summaries)}
    """
    try:
        res = clova_client.chat.completions.create(
            model="HCX-005",
            messages=[{"role": "user", "content": final_prompt}]
        )
        return res.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ ìµœì¢… í†µí•© ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "\n\n".join(partial_summaries)

# ================================
# âœ… 3. ê³µì‹œ + ë‰´ìŠ¤ â†’ RAG docs
# ================================
def analyze_disclosure_with_rag(corp_name, report_nm, rcept_no):
    """
    âœ… ê³µì‹œ ë³¸ë¬¸ + í‘œ â†’ chunk ìš”ì•½ â†’ ë‰´ìŠ¤ ê²°í•© â†’ RAG ì‹œë‚˜ë¦¬ì˜¤
    """
    # 1) ê³µì‹œ ë³¸ë¬¸ + í‘œ ë°ì´í„°
    disclosure_raw = fetch_disclosure_with_tables(rcept_no)
    if not disclosure_raw:
        return "âš ï¸ ê³µì‹œ ì›ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # âœ… ê¸¸ë©´ chunk ìš”ì•½ ì ìš©
    disclosure_text = safe_summarize_large_text(disclosure_raw)

    # 2) ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§
    keyword = f"{corp_name} {report_nm}"
    news_results = crawl_naver_view_titles(keyword, limit=5) or []
    news_docs = [n.get("preview") for n in news_results if n.get("preview")]

    # 3) docs ê²°í•©
    docs = []
    docs.append(f"[ê³µì‹œ ìš”ì•½] {corp_name} - {report_nm}\n{disclosure_text}")
    docs.extend([f"[ë‰´ìŠ¤] {nd}" for nd in news_docs])

    if not docs:
        return "âš ï¸ ê³µì‹œ/ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # âœ… ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
    save_path = f"embeddings/{corp_name}_mix.faiss"
    create_faiss_index_from_docs(docs, save_path=save_path)

    # âœ… í”„ë¡¬í”„íŠ¸
    query = f"""
    {corp_name}ì˜ '{report_nm}' ê³µì‹œì—ì„œ **í‘œ ì•ˆì˜ ìž¬ë¬´ ìˆ«ìž**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ
    - ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ, ì „ë…„ë™ê¸°/ì „ë¶„ê¸° ëŒ€ë¹„ ë³€í™”ìœ¨
    - ì‹œìž¥ ì»¨ì„¼ì„œìŠ¤ ëŒ€ë¹„ í‰ê°€
    - ì£¼ê°€ ì˜í–¥ ì‹œë‚˜ë¦¬ì˜¤ (í˜¸ìž¬/ì¤‘ë¦½/ì•…ìž¬)
    - ê´€ë ¨ ì„¹í„° ëŒ€ì²´ ì „ëžµ
    ìš”ì•½í•´ì¤˜.
    """
    return rag_query_from_docs(query, docs, save_path)

# ================================
# âœ… 4. ìƒìž¥ì‚¬ ë¦¬ìŠ¤íŠ¸ & ìµœê·¼ ê³µì‹œ í•„í„°
# ================================
def get_corp_list(save_path="data/corp_list.csv"):
    """
    âœ… DART ì „ì²´ ìƒìž¥ì‚¬ ë¦¬ìŠ¤íŠ¸ (corp_code ë§¤í•‘)
    """
    url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={DART_API_KEY}"
    res = requests.get(url)
    if res.status_code != 200:
        raise Exception("DART API ì—°ê²° ì‹¤íŒ¨")

    with zipfile.ZipFile(io.BytesIO(res.content)) as z:
        xml_file = z.open(z.namelist()[0]).read()

    root = ET.fromstring(xml_file)
    rows = []
    for row in root.findall("list"):
        rows.append([
            row.find("corp_code").text,
            row.find("corp_name").text,
            row.find("stock_code").text
        ])

    import pandas as pd
    df = pd.DataFrame(rows, columns=["corp_code", "corp_name", "stock_code"])
    os.makedirs("data", exist_ok=True)
    df.to_csv(save_path, index=False, encoding="utf-8-sig")
    return df

def get_recent_disclosures(corp_code=None, start_date=None, end_date=None, page_count=50):
    """
    âœ… ìµœê·¼ â€˜ì‹¤ì  ê´€ë ¨â€™ ê³µì‹œë§Œ í•„í„°
    """
    if not start_date:
        start_date = (datetime.date.today() - datetime.timedelta(days=90)).strftime("%Y%m%d")
    if not end_date:
        end_date = datetime.date.today().strftime("%Y%m%d")

    url = f"https://opendart.fss.or.kr/api/list.json?crtfc_key={DART_API_KEY}&bgn_de={start_date}&end_de={end_date}&page_count={page_count}"
    if corp_code:
        url += f"&corp_code={corp_code}"

    res = requests.get(url).json()
    if res.get("status") != "000":
        print(f"âš ï¸ DART API ì˜¤ë¥˜: {res.get('message')}")
        return []

    keywords = ["ë¶„ê¸°ë³´ê³ ì„œ", "ë°˜ê¸°ë³´ê³ ì„œ", "ì‚¬ì—…ë³´ê³ ì„œ", "ìž ì •ì‹¤ì "]
    filtered = []
    for d in res.get("list", []):
        if any(k in d["report_nm"] for k in keywords):
            dt = d["rcept_dt"]
            filtered.append({
                "corp_name": d["corp_name"],
                "report_nm": d["report_nm"],
                "rcept_dt": dt,
                "rcept_no": d["rcept_no"],
                "url": f"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={d['rcept_no']}"
            })
    return filtered